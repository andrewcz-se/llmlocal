# llmlocal
Run a local LLM (via Ollama) and query it with a Python chat script
